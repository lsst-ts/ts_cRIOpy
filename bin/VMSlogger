#!/usr/bin/env python3.8

# Save VMS data to a file.
#
# Developed for the LSST Telescope and Site Systems.
# This product includes software developed by the LSST Project
# (https://www.lsst.org). See the COPYRIGHT file at the top - level directory
# of this distribution for details of code ownership.
#
# This program is free software : you can redistribute it and / or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option) any
# later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE.See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program.If not, see <https://www.gnu.org/licenses/>.

import os
import signal
import sys

from lsst.ts.salobj import Domain, Remote
from lsst.ts.cRIOpy.VMSCache import *

import argparse
import asyncio
import click
from datetime import datetime
import numpy as np
import pathlib
import logging

try:
    import h5py

    has_h5py = True
except ModuleNotFoundError:
    has_h5py = False


devices = ["M1M3", "M2", "Rotator"]

parser = argparse.ArgumentParser(description="Save VMS data to a file")
parser.add_argument(
    "devices", type=str, nargs="+", help="name or index of CSC", choices=devices
)
parser.add_argument(
    "-5",
    dest="h5py",
    action="store_true",
    help="save into HDF 5. Requires h5py (pip install h5py)",
)
parser.add_argument("--chunk-size", dest="chunk_size", default=5000, type=int)
parser.add_argument(
    "-d", dest="debug", default=0, action="count", help="increase debug level"
)
parser.add_argument(
    "--header", dest="header", action="store_true", help="adds header with column names"
)
parser.add_argument("-p", dest="pidfile", action="store", help="PID file location")
parser.add_argument(
    "-s", type=int, dest="size", default=None, help="number of records to save"
)
parser.add_argument(
    "-z", action="store_true", dest="zip_file", help="gzip output files"
)
parser.add_argument(
    "--workdir",
    action="store",
    dest="workdir",
    default="",
    help="directory where files will be stored",
)
parser.add_argument(
    "--logfile",
    action="store",
    dest="logfile",
    default=None,
    help="write log messages to given file",
)
parser.add_argument(
    "--daemon",
    action="store_true",
    dest="daemon",
    help="starts as daemon (fork to start process).",
)

device_sensors = [3, 6, 3]

logger = logging.getLogger("VMSlogger")


class Collector:
    def __init__(
        self, index, fn_template, size, file_type, header, chunk_size, daemonized
    ):
        self.index = index
        self.fn_template = fn_template
        self.size = size
        self.file_type = file_type
        self.header = header
        self.daemonized = daemonized
        self.h5file = None

        logger.debug(
            f"Creating cache: index={self.index+1} device={device_sensors[self.index]} type={self.file_type}"
        )

        if "5" in self.file_type:
            self.chunk_size = min(chunk_size, self.size)
        else:
            self.chunk_size = self.size

        self.cache_size = self.chunk_size + 50000

        self.cache = VMSCache(self.cache_size, device_sensors[self.index])

        self._create_file()

    def _create_file(self):
        filename = datetime.strftime(datetime.now(), self.fn_template)
        logger.info(f"Creating {filename}")

        if "5" in self.file_type:
            self.h5file = h5py.File(filename, "a")
            group_args = {"chunks": (self.chunk_size)}
            if "z" in self.file_type:
                group_args["compression"] = "gzip"
            self.cache.create_hdf5_datasets(self.size, self.h5file, group_args)
        else:
            self.filename = filename

    def _save_hdf5(self):
        if self.h5file is not None and len(self.cache) >= self.chunk_size:
            logger.debug(
                f"Saving device {devices[self.index]} data to {self.h5file.file.filename} from {self.cache.hdf5_index}"
            )
            self.cache.savehdf5(self.chunk_size)
            self.h5file.flush()
            return True
        return False

    def close(self):
        if self.h5file is not None:
            logger.info(f"Closing HDF5 {self.h5file.file.filename}")
            self.h5file.close()

    async def sample_file(self):
        if self.daemonized or logger.getEffectiveLevel() == logging.DEBUG:
            saved_len = 0
            while True:
                l = saved_len + len(self.cache)
                logger.debug(
                    f"Waiting {devices[self.index]}..{100 * (l)/self.size:.02f}% {l} of {self.size}"
                )
                if self.h5file is None:
                    if l >= self.size:
                        break
                else:
                    if self._save_hdf5():
                        saved_len += self.chunk_size
                    if self.cache.h5_filled():
                        break
                await asyncio.sleep(0.5)
        else:

            async def collect_it(bar):
                last_l = 0

                while True:
                    l = len(self.cache)
                    if l >= self.size:
                        break
                    bar.update(l - last_l)
                    last_l = l
                    await asyncio.sleep(0.1)
                    if self._save_hdf5():
                        bar.update(self.chunk_size - last_l)
                        break

            with click.progressbar(
                length=self.size,
                label=f"Getting data {devices[self.index]}",
                show_eta=True,
                show_percent=True,
                width=0,
            ) as bar:
                if self.h5file is None:
                    await collect_it(bar)
                else:
                    while True:
                        await collect_it(bar)
                        if self.cache.h5_filled():
                            break

                bar.update(self.size)

        if self.h5file is None:
            logger.info(f"Saving CSV to {self.filename}")
            kwargs = {"delimiter": ","}
            if self.header:
                kwargs["header"] = ",".join(self.cache.columns())
            self.cache.savetxt(self.filename, self.size, **kwargs)

    async def collect_data(self):
        async with Domain() as domain:
            remote = Remote(domain, "MTVMS", index=self.index + 1)
            remote.tel_data.callback = lambda data: self.cache.newChunk(data, 0.001)
            while True:
                await self.sample_file()
                self._create_file()


async def main(args, pipe=None):
    logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()

    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    ch.setFormatter(formatter)

    if args.logfile:
        fh = logging.FileHandler(args.logfile)
        fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    if args.debug > 0:
        ch.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
        ch.setLevel(logging.INFO)
    logger.addHandler(ch)

    tasks = []
    collectors = []

    def cancel_all(signum, frame):
        logger.info(f"Canceling after {signum}")
        for t in tasks:
            t.cancel()

    for signum in [signal.SIGINT, signal.SIGHUP, signal.SIGTERM]:
        signal.signal(signum, cancel_all)

    file_type = ""
    if args.zip_file:
        file_type += "z"
    if args.h5py:
        if has_h5py is False:
            logger.error(
                "Python is missing h5py module, saving HDF 5 file is not supported. Please install h5py first (pip install h5py)."
            )
            sys.exit(1)
        file_type += "5"
        if args.size is None:
            args.size = 86400000
    else:
        if args.size is None:
            args.size = 50000

    if args.pidfile:
        f = open(args.pidfile, "w")
        f.write(f"{os.getpid()}\n")
        f.close()

    for d in args.devices:
        fn = d + "_%Y-%m-%dT%H:%M:%S"
        if "5" in file_type:
            fn += ".hdf"
        else:
            fn += ".csv"
            if "z" in file_type:
                fn += ".gz"

        fn = str(pathlib.Path(args.workdir, fn))

        logger.info(f"Collecting {d} to {fn}")
        c = Collector(
            devices.index(d),
            fn,
            args.size,
            file_type,
            args.header,
            args.chunk_size,
            args.daemon,
        )
        collectors.append(c)
        tasks.append(asyncio.create_task(c.collect_data()))

    if pipe is not None:
        os.write(pipe, b"OK\n")
        os.close(pipe)
    try:
        await asyncio.gather(*tasks)
        logger.info("Done")
    except asyncio.exceptions.CancelledError:
        logger.info("Canceled")

    for c in collectors:
        c.close()


args = parser.parse_args()
if args.daemon:
    import time

    r_pipe, w_pipe = os.pipe2(os.O_NONBLOCK)
    child = os.fork()
    if child == 0:
        os.close(0)
        os.close(1)
        os.close(2)

        dn = os.open("/dev/null", os.O_WRONLY)
        os.dup(dn)
        os.dup(dn)
        os.dup(dn)

        asyncio.run(main(args, pipe=w_pipe))
    else:
        time.sleep(1)
        ret = os.read(r_pipe, 50)
        os.close(r_pipe)
        if ret == b"OK\n":
            sys.exit(0)

        print("Returned: ", ret)
        sys.exit(1)
else:
    asyncio.run(main(args))
