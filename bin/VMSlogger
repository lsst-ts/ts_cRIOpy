#!/usr/bin/env python3.8

# Save VMS data to a file.
#
# Developed for the LSST Telescope and Site Systems.
# This product includes software developed by the LSST Project
# (https://www.lsst.org). See the COPYRIGHT file at the top - level directory
# of this distribution for details of code ownership.
#
# This program is free software : you can redistribute it and / or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option) any
# later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE.See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program.If not, see <https://www.gnu.org/licenses/>.

import os
import os.path
import signal
import sys
import traceback

from lsst.ts.salobj import Domain, Remote
from lsst.ts.cRIOpy.VMSCache import VMSCache

import argparse
import asyncio
import click
from datetime import datetime
import pathlib
import logging

try:
    import h5py

    has_h5py = True
except ModuleNotFoundError:
    has_h5py = False


devices = ["M1M3", "M2", "Rotator"]

parser = argparse.ArgumentParser(
    description="Save VMS data to a file, either HDF5 or CSV.",
    epilog="Data are read as they arrive in DDS messages, matched by timestamps. Only complete (from all accelerometers the device provides) records are stored. Allows either single or multiple devices recording. Can be launched as daemon, running on background. Recorded data can be analysed offline with VMSGUI.",
)
parser.add_argument(
    "devices", type=str, nargs="+", help="name or index of CSC", choices=devices
)
parser.add_argument(
    "-5",
    dest="h5py",
    action="store_true",
    help="save into HDF 5. Requires h5py (pip install h5py). Save to CSV if not provided.",
)
parser.add_argument(
    "--chunk-size",
    dest="chunk_size",
    default=5000,
    type=int,
    help="receiving chunk size. After receiving this number of records, data are added to HDF5 file (and the file flushed to disk). Has no effect if CSV (default) format is used. Defaults to 5000.",
)
parser.add_argument(
    "-d", dest="debug", default=0, action="count", help="increase debug level"
)
parser.add_argument(
    "--header", dest="header", action="store_true", help="adds header with column names"
)
parser.add_argument("-p", dest="pidfile", action="store", help="PID file location")
parser.add_argument(
    "-s", type=int, dest="size", default=None, help="number of records to save"
)
parser.add_argument(
    "-z", action="store_true", dest="zip_file", help="gzip output files"
)
parser.add_argument(
    "--single-shot",
    action="store_true",
    dest="single_shot",
    help="quit after recording single file (with -s records)",
)
parser.add_argument(
    "--template",
    action="store",
    default="${name}_%Y-%m-%dT%H:%M:%S.${ext}",
    type=str,
    help="template used to construct filename",
)
parser.add_argument(
    "--workdir",
    action="store",
    dest="workdir",
    default="",
    help="directory where files will be stored",
)
parser.add_argument(
    "--logfile",
    action="store",
    dest="logfile",
    default=None,
    help="write log messages to given file",
)
parser.add_argument(
    "--daemon",
    action="store_true",
    dest="daemon",
    help="starts as daemon (fork to start process).",
)

device_sensors = [3, 6, 3]

logger = logging.getLogger("VMSlogger")


class Collector:
    def __init__(
        self, index, fn_template, size, file_type, header, chunk_size, daemonized
    ):
        self.index = index
        self.fn_template = fn_template
        self.size = size
        self.file_type = file_type
        self.header = header
        self.daemonized = daemonized
        self.h5file = None

        logger.debug(
            f"Creating cache: index={self.index+1} device={device_sensors[self.index]} type={self.file_type}"
        )

        if "5" in self.file_type:
            self.chunk_size = min(chunk_size, self.size)
        else:
            self.chunk_size = self.size

        self.cache_size = self.chunk_size + 50000

        self.cache = VMSCache(self.cache_size, device_sensors[self.index])

    def get_filename(self):
        filename = datetime.strftime(datetime.now(), self.fn_template)
        repl = [
            ("name", devices[self.index]),
            (
                "ext",
                "hdf"
                if "5" in self.file_type
                else "csv.gz"
                if "z" in self.file_type
                else "csv",
            ),
        ]
        for name, value in repl:
            filename = filename.replace("${" + name + "}", value)
        return filename

    def _create_file(self):
        filename = self.get_filename()
        logger.info(f"Creating {filename}")

        try:
            dirs = os.path.dirname(filename)
            if dirs != "":
                os.makedirs(dirs)
        except FileExistsError:
            pass

        if "5" in self.file_type:
            self.h5file = h5py.File(filename, "a")
            group_args = {"chunks": (self.chunk_size)}
            if "z" in self.file_type:
                group_args["compression"] = "gzip"
            self.cache.create_hdf5_datasets(self.size, self.h5file, group_args)
        else:
            self.filename = filename

    def _save_hdf5(self):
        if self.h5file is None or len(self.cache) < self.chunk_size:
            return False
        logger.debug(
            f"Saving device {devices[self.index]} data to {self.h5file.file.filename} from {self.cache.hdf5_index}"
        )
        self.cache.savehdf5(self.chunk_size)
        self.h5file.flush()
        return True

    def close(self):
        if self.h5file is not None:
            logger.info(f"Closing HDF5 {self.h5file.file.filename}")
            self.h5file.close()

    async def _sample_daemon(self):
        saved_len = 0
        while True:
            current_len = saved_len + len(self.cache)
            logger.debug(
                f"Waiting {devices[self.index]}..{100 * (current_len)/self.size:.02f}% {current_len} of {self.size}"
            )
            if self.h5file is None:
                if current_len >= self.size:
                    break
            else:
                if self._save_hdf5():
                    saved_len += self.chunk_size
                if self.cache.h5_filled():
                    break
            await asyncio.sleep(0.5)

    async def _sample_cli(self):
        async def collect_it(bar):
            last_l = 0

            while True:
                cache_len = len(self.cache)
                if cache_len >= self.size:
                    break
                bar.update(cache_len - last_l)
                last_l = cache_len
                await asyncio.sleep(0.1)
                if self._save_hdf5():
                    bar.update(self.chunk_size - last_l)
                    break

        with click.progressbar(
            length=self.size,
            label=f"Getting data {devices[self.index]}",
            show_eta=True,
            show_percent=True,
            width=0,
        ) as bar:
            if self.h5file is None:
                await collect_it(bar)
            else:
                while True:
                    await collect_it(bar)
                    if self.cache.h5_filled():
                        break

            bar.update(self.size)

    async def sample_file(self):
        if self.daemonized or logger.getEffectiveLevel() == logging.DEBUG:
            await self._sample_daemon()
        else:
            await self._sample_cli()

        if self.h5file is not None:
            return

        logger.info(f"Saving CSV to {self.filename}")
        kwargs = {"delimiter": ","}
        if self.header:
            kwargs["header"] = ",".join(self.cache.columns())
        self.cache.savetxt(self.filename, self.size, **kwargs)

    async def collect_data(self, single_shot):
        """Create data files, filled them with data.

        Parameters
        ----------
        single_shot : `bool`
            When True, quits after single file is recorded.
        """
        try:
            async with Domain() as domain:
                remote = Remote(domain, "MTVMS", index=self.index + 1)
                remote.tel_data.callback = lambda data: self.cache.newChunk(data, 0.001)
                while True:
                    self._create_file()
                    await self.sample_file()
                    if single_shot:
                        break

        except Exception:
            logger.error(f"Cannot collect data for {devices[self.index]}")
            for line in traceback.format_exc().split("\n"):
                logger.error(f"> {line}")


async def main(args, pipe=None):
    logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()

    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    ch.setFormatter(formatter)

    if args.logfile:
        fh = logging.FileHandler(args.logfile)
        fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    if args.debug > 0:
        ch.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
        ch.setLevel(logging.INFO)
    logger.addHandler(ch)

    tasks = []
    collectors = []

    def cancel_all(signum, frame):
        logger.info(f"Canceling after {signum}")
        for t in tasks:
            t.cancel()

    for signum in [signal.SIGINT, signal.SIGHUP, signal.SIGTERM]:
        signal.signal(signum, cancel_all)

    file_type = ""
    if args.zip_file:
        file_type += "z"
    if args.h5py:
        if has_h5py is False:
            logger.error(
                "Python is missing h5py module, saving HDF 5 file is not supported. Please install h5py first (pip install h5py)."
            )
            sys.exit(1)
        file_type += "5"
        if args.size is None:
            args.size = 86400000
    else:
        if args.size is None:
            args.size = 50000

    if args.pidfile:
        f = open(args.pidfile, "w")
        f.write(f"{os.getpid()}\n")
        f.close()

    for d in args.devices:
        fn = d + "_%Y-%m-%dT%H:%M:%S"
        if "5" in file_type:
            fn += ".hdf"
        else:
            fn += ".csv"
            if "z" in file_type:
                fn += ".gz"

        fn = str(pathlib.Path(args.workdir, fn))

        logger.info(f"Collecting {d} to {fn}")
        c = Collector(
            devices.index(d),
            fn,
            args.size,
            file_type,
            args.header,
            args.chunk_size,
            args.daemon,
        )
        collectors.append(c)
        tasks.append(asyncio.create_task(c.collect_data(args.single_shot)))

    if pipe is not None:
        os.write(pipe, b"OK\n")
        os.close(pipe)
    try:
        await asyncio.gather(*tasks)
        logger.info("Done")
    except asyncio.exceptions.CancelledError:
        logger.info("Canceled")

    for c in collectors:
        c.close()


args = parser.parse_args()
if args.daemon:
    import time

    r_pipe, w_pipe = os.pipe2(os.O_NONBLOCK)
    child = os.fork()
    if child == 0:
        os.close(0)
        os.close(1)
        os.close(2)

        dn = os.open("/dev/null", os.O_WRONLY)
        os.dup(dn)
        os.dup(dn)
        os.dup(dn)

        asyncio.run(main(args, pipe=w_pipe))
    else:
        time.sleep(1)
        ret = os.read(r_pipe, 50)
        os.close(r_pipe)
        if ret == b"OK\n":
            sys.exit(0)

        print("Returned: ", ret)
        sys.exit(1)
else:
    asyncio.run(main(args))
